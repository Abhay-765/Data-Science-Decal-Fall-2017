{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "from pylab import *\n",
    "from numpy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import scipy.io as io\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Principal Component Analysis (PCA):\n",
    "\n",
    "1.1: Visualize eigenvectors (different components) of PCA and see how the clusters change. Do this with the provided, labeled mnist dataset.\n",
    "Each label should have its own color (10 colors total for 10 digits). Graph $X_0, ..., X_9$ where $X_i$ is the input for images with label i (digit i). in 2 dimensions by picking 2 principal components at a time. Then graph X in 3 dimensions. Note: Graph the first 60 images. \n",
    "\n",
    "Sklearn function: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "You should see that picking different principal components not only explains different variations, but also separates the distributions, or clusters the points over different axis. PCA is useful in clustering when there are too many features, or too many dimensions to cluster over, so we'd rather reduce the dimension of the data or pick which dimensions we want to represent the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.  0.  0. ...,  9.  6.  3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import MNIST\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    data = io.loadmat('mnist_data/images.mat') \n",
    "    images = data['images']\n",
    "    X = np.zeros((images.shape[2], 784))\n",
    "    for i in range(images.shape[2]):\n",
    "        X[i] = images[:,:,i].flatten()\n",
    "    # make them all the same cluster/color first\n",
    "    return X, np.zeros(X.shape[0]), images.T\n",
    "\n",
    "def one_hot(labels_train):\n",
    "    '''Convert categorical labels 0,1,2,....9 to standard basis vectors in R^{10} '''\n",
    "    return np.eye(NUM_CLASSES)[labels_train]\n",
    "\n",
    "def shuffle(Xtrain, ytrain):\n",
    "    stacked = np.column_stack((Xtrain,ytrain))\n",
    "    np.random.shuffle(stacked)\n",
    "    return stacked[:,:Xtrain.shape[1]], stacked[:,Xtrain.shape[1]]\n",
    "\n",
    "def load_labeled_dataset():\n",
    "    mndata = MNIST('mnist_data/raw_data/')\n",
    "    X_train, labels_train = map(np.array, mndata.load_training())\n",
    "    X_train = (X_train - np.mean(X_train)) / 255 #np.std(X_train) # or / 255\n",
    "    X_train, labels_train = shuffle(X_train, labels_train.astype(float))\n",
    "    \n",
    "    y_train = one_hot(labels_train.astype(int))\n",
    "    return X_train, y_train, labels_train\n",
    "\n",
    "\n",
    "X, y, labels_train = load_labeled_dataset()\n",
    "print labels_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "# Compute the PC's of X\n",
    "\n",
    "color=cm.rainbow(np.linspace(0,1,NUM_CLASSES))\n",
    "assigned_colors = [color[int(labels_train[i])] for i in range(labels_train.shape[0])]\n",
    "# plot the first two PC's\n",
    "\n",
    "\n",
    "# plot the first three PC's\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Singular Value Decomposition (SVD)\n",
    "\n",
    "2.1: Visualize low rank approximation by graphing the percentage of variation in the data. Pick a random mnist image, then get the SVD of that one image. Graph the original image. Then graph the approximated image with rank 1, 2, ..., 28 (full rank). \n",
    "\n",
    "For example, the approximated image for rank 1 is:\n",
    "\n",
    "Given $U$ (28x28), $S $ (28x28) diagonal, $V^{\\top} $ (28x28),\n",
    "\n",
    "$U[:,:1]S[:1,:1]V[:1]^{\\top}$\n",
    "\n",
    "Numpy function: https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html\n",
    "\n",
    "Image plotting demos: https://matplotlib.org/users/image_tutorial.html\n",
    "\n",
    "The directions that capture the most variation (descending) are the principle components with the largest eigenvalues (descending). These directions most define the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newX, filler_labels, images = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = images[np.random.randint(images.shape[0])].T\n",
    "\n",
    "# Compute the SVD\n",
    "\n",
    "# Show the original image\n",
    "    \n",
    "# Show the approximated image for rank 1, .., 27 (inclusive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2: Whitening\n",
    "\n",
    "Compute the low rank approximation, then graph the image without the singular values, thus weighting each orthonormal direction the same. There should be 28 approximated images for rank 1, 2, ..., 28, with rank 28 being that of the original image.\n",
    "\n",
    "The purpose of this is to emphasize weaker signals, which is especially helpful if your model trains superbly well on datapoints that happen too frequently (which at this point is redundant learning) and performs poorly on other datapoints. This forces the model to learn each direction / type of data more equally (or with equal consideration). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show the approximated image without singular values for rank 1, .., 27 (inclusive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional References:\n",
    "https://people.eecs.berkeley.edu/~jrs/papers/machlearn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
